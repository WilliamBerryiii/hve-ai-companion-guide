---
title: Teaching Scenario - Cost of Skipping Engineering Fundamentals
description: Explore a teaching scenario illustrating how weak engineering fundamentals blocked AI effectiveness and the transformation after improvement
author: HVE Core Team
ms.date: 2025-11-18
chapter: 1
part: I
keywords:
  - teaching-scenario
  - engineering-fundamentals
  - ai-effectiveness
  - composite-example
  - lessons-learned
---

> [!TIP]
> **Already understand the Four Pillars?**
>
> If you're ready to assess your own codebase, skip ahead to [The Assessment Framework](04-assessment-framework.md). This scenario shows the concepts in action but isn't required for the hands-on exercise.

<!-- markdownlint-disable MD028 -->

> [!NOTE]
> **Teaching Scenario**
>
> This scenario is a composite teaching example, not a documented case study. The character, company, and timeline are fictional but illustrate patterns commonly observed in AI adoption without engineering fundamentals. Names, details, and specific circumstances are designed for instructional clarity.

<!-- markdownlint-enable MD028 -->

Meet Sarah, a senior engineer at a mid-size SaaS company. She's talented, experienced, and excited about GitHub Copilot. Her team adopted it three months ago, and she's seen impressive code generationâ€”when it works. Today, she's about to discover why "when it works" is the problem.

## Scenario: MidSize SaaS Company

* **Team**: 8 developers led by Sarah as technical lead
* **Product**: B2B project management platform (Node.js/React)
* **Challenge**: Real-time notifications system needed for upcoming client demo
* **Context**: Team adopted GitHub Copilot 3 months ago without improving engineering fundamentals

**Timeline**: 2 weeks estimated (Sarah's confident they can deliver)

**Initial Codebase State:**

* **Tests**: âŒ 8% code coverage (mostly outdated)
* **Linting**: âš ï¸ ESLint installed but not enforced (800+ warnings)
* **Documentation**: âŒ Outdated README, no inline comments, no ADRs
* **Source Control**: âŒ "fix", "update", "WIP" commit messages

**Sarah's Thought Process**: "We've got Copilot. It's been generating solid code for months. Two weeks should be plenty for a notifications system."

You might recognize this optimism. Many teams feel this way when AI assistance is new and impressive. Let's see what happens when that optimism meets reality.

## Week 1: AI-assisted development begins

### Day 1-2: Initial Implementation - The Honeymoon Phase

#### Sarah's Experience

Sarah opened GitHub Copilot Chat with confidence. "I need a WebSocket server for real-time notifications," she prompted. Within minutes, Copilot generated a complete implementation. She reviewed the code and it looked clean, idiomatic, exactly what she would have written herself.

"This is exactly why we adopted Copilot," she thought, committing the code. "Two weeks? We might finish in one."

If you've used AI assistance before, you know this feeling. That moment when the AI *just gets it*, and you think, "This is going to change everything."

Here's the code Copilot generated:

```javascript
// Generated by Copilot (looks good!)
const io = require('socket.io')(server);

io.on('connection', (socket) => {
  socket.on('subscribe', (userId) => {
    socket.join(`user_${userId}`);
  });
  
  socket.on('disconnect', () => {
    // Copilot generates cleanup code
    socket.rooms.forEach(room => socket.leave(room));
  });
});
```

#### Problem 1: No tests to validate behavior

* Developer assumes code works
* No validation that subscription logic functions
* No test for disconnect cleanup

#### Problem 2: Inconsistent with codebase patterns

* Existing code uses `socket.io-redis` for multi-server support
* Copilot suggested single-server approach (no Redis adapter)
* No documentation of existing Socket.io patterns to guide Copilot

**Result**: Code looks correct but breaks in production, a multi-server environment

---

### Day 3-4: Notification Trigger Logic - First Doubts

#### Sarah's Day 3-4 Experience

Feeling confident from Day 1's success, Sarah moved to the notification triggers. She asked Copilot to generate the code that would send notifications when orders were created. The AI responded instantly with clean, working code.

But something nagged at her. The code looked *different* from how the team usually handled cross-cutting concerns. "Maybe this is a better pattern," she rationalized. "Copilot has seen thousands of codebases. It probably knows better approaches than our old conventions."

This is a critical moment you might recognize: when AI suggests something that *looks* right but doesn't quite fit your architecture. Without documentation to guide the AI, and without tests to validate the approach, it's hard to know which pattern to trust.

Here's what Copilot generated:

```javascript
// Generated by Copilot
async function createOrder(orderData) {
  const order = await Order.create(orderData);
  
  // Notify user
  io.to(`user_${order.userId}`).emit('notification', {
    type: 'order_created',
    orderId: order.id
  });
  
  return order;
}
```

#### Problem 3: No architectural guidance

* Tightly couples order creation to Socket.io
* No error handling if Socket.io fails
* Existing codebase uses event bus pattern (not documented)
* Copilot doesn't know to follow event bus pattern

#### Problem 4: No linting enforcement

* Code uses inconsistent promise handling (mix of async/await and .then())
* Variables named inconsistently (order vs orderData vs data)
* No pre-commit hooks caught these issues

**Result**: More inconsistency added to already-inconsistent codebase

---

### Day 5: Integration Testing Reveals Issues - Reality Hits

#### Sarah's Day 5 Experience

Sarah deployed to staging with the confidence of someone who'd reviewed every line of AI-generated code. She ran through the manual test checklist. Local dev worked perfectly. Then she deployed to staging.

Nothing worked.

Notifications failed. Order creation threw errors. The staging servers (load-balanced across multiple instances) exposed every assumption the AI had made about single-server deployments.

"How did I miss this?" Sarah stared at the staging logs. Eight hours later, she'd traced the problems:

* âœ… Notifications work in local dev (single server)
* âŒ Notifications fail in staging (multi-server, load balanced)
* âŒ Order creation fails if Socket.io server down
* âŒ Memory leaks in disconnection handling

#### The Breakthrough Moment

Sarah realized something important: Copilot had generated *exactly* what it thought was correct based on the code it could see. But without tests to validate multi-server behavior, without documentation explaining the architecture, without Git history showing the event bus pattern, Copilot had no way to know what "correct" meant for *this* codebase.

"The AI isn't broken," Sarah thought. "Our fundamentals are."

**Time spent debugging**: 8 hours (entire day)

**Root causes**:

1. No tests caught single-server assumption
2. No docs explained multi-server architecture
3. No Git history showed event bus pattern
4. Copilot amplified poor architectural patterns

## Week 2: Rework and firefighting

### Day 6-7: Refactoring - The Grind

**Sarah's Experience:**

The client demo was 8 days away. Sarah's optimistic "we might finish in one week" felt like a distant memory. Now she faced a harder truth: she'd have to rebuild significant portions of the implementation.

What frustrated her most wasn't the rework itself, every developer has been there. It was realizing that the AI had *accelerated* her in the wrong direction. She'd moved fast, but without the fundamentals in place to validate correctness, "fast" just meant "fast toward problems."

She made a list of what needed fixing:

* Add Redis adapter for multi-server Socket.io
* Implement event bus pattern for notification triggers
* Add error handling and retries
* Write tests (should have been first!)

**Time spent**: 12 hours (rework that could have been prevented)

---

### Day 8-9: Testing and Documentation - Should Have Started Here

**Sarah's Experience:**

"I should have written these first," Sarah thought, adding test after test to validate the refactored code. Each test that passed gave her a small measure of confidence. Each test that failed revealed another assumption she'd made.

The irony wasn't lost on her: she was now doing manually what should have been automated from the start. Writing tests, adding documentation, explaining the architecture that Copilot should have learned from initially.

What she added:

* Integration tests for notification flow
* Documentation of Socket.io architecture
* Inline comments explaining multi-server concerns

**Time spent**: 10 hours (should have been 2-3 if done incrementally)

---

### Day 10: Production Rollout Disaster - Rock Bottom

**Sarah's Experience:**

Sarah watched the production metrics spike. Memory usage climbing. Servers crashing. Rollback initiated. The client demo? Delayed a week.

She'd found the root cause within an hour ... a subtle bug in the disconnect cleanup logic Copilot had generated on Day 1. The code had *looked* correct. Even her careful review had missed it. But tests would have caught it immediately.

This was the moment Sarah understood: without engineering fundamentals, AI assistance wasn't just unhelpful ... it was *dangerous*. It lets you move fast enough to cause real damage.

* Deploy to production
* Memory leaks cause servers to crash
* Root cause: Disconnect cleanup logic (from Copilot) has bug
* Rollback required
* Client demo delayed one week

**Time spent**: 6 hours of incident response + 1 week delayed launch

**Team Impact**: Trust in AI assistance shaken, Sarah's confidence bruised, client relationship stressed.

## The Turnaround: Adding Fundamentals

**Sarah's Decision:**

After the incident, Sarah made a proposal to her team: "Before we use AI for the next feature, let's fix our fundamentals. Three weeks of investment. Testing, linting, documentation, source control hygiene. Then we try again."

The team agreed. What they accomplished in those three weeks transformed how they worked with AIâ€”and how AI worked for them.

**Post-Incident Actions:**

### 1. Testing (2 weeks investment)

* Add test coverage for critical paths (goal: 30%)
* Set up Jest + Supertest for integration tests
* CI configured to require tests for new code

### 2. Linting (1 day investment)

* Configure ESLint + Prettier with team-agreed rules
* Add pre-commit hooks (Husky)
* Fix top 50 most common violations in critical files
* CI fails on linting errors

### 3. Documentation (3 days investment)

* Update README with architecture overview
* Document Socket.io multi-server setup
* Add inline comments to complex notification logic
* Create ADR for event bus pattern choice

### 4. Source Control (1 day investment)

* Establish conventional commits standard
* Create PR template with checklist
* Require 1 reviewer approval before merge

**Total investment**: ~3 weeks of gradual improvement

## Three Months Later: Same Feature, Different Results

**New Feature Request**: Add in-app chat functionality (similar complexity)

**Sarah's Mindset**: Three months ago, she would have jumped straight into implementation. Now? She started with research. The difference was remarkable.

**Codebase State Now:**

* **Tests**: âœ… 35% coverage (critical paths covered)
* **Linting**: âœ… Enforced with pre-commit hooks
* **Documentation**: âœ… README + ADRs + inline comments
* **Source Control**: âœ… Conventional commits + PR reviews

### Development with AI

#### Day 1: Research

* Developer checks existing Socket.io documentation (in README)
* Understands multi-server architecture (documented in ADR-004)
* Reviews event bus pattern (used in notification system)

#### Day 2-3: Implementation

* Copilot suggests code following documented patterns
* Pre-commit hooks enforce consistent style
* Tests written alongside implementation (TDD)

#### Day 4: Integration Testing

* All tests pass (including new integration tests)
* Manual testing in staging confirms multi-server support works
* No surprises

#### Day 5: Production Rollout - Victory

**Sarah's Experience:**

Sarah watched the production deployment with cautious optimism. Metrics looked good. No errors. Memory usage stable. Twenty minutes later, she allowed herself to relax.

"This is what AI assistance is supposed to feel like," she thought. Confident. Reliable. *Fast* in a sustainable way, not just in the moment.

The chat feature worked perfectly:

* Smooth deployment
* No incidents
* Monitoring shows healthy performance
* Tests validated every component
* Code followed team patterns automatically

**Total time**: Completed within original estimate
**AI effectiveness**: Highly effective assistance (vs. counterproductive originally)

## Lessons Learned

**What Sarah and Her Team Discovered:**

Three months after the incident, Sarah reflected on what they'd learned. These insights transformed how they thought about AI assistance.

You might recognize your own team's journey in some of these discoveries:

1. **AI amplifies existing quality** (good or bad)
   * Poor fundamentals â†’ AI suggestions compound problems
   * Good fundamentals â†’ AI suggestions improve code

2. **Upfront investment pays dividends**
   * 3 weeks fundamentals work â†’ noticeably faster feature development
   * Investment typically pays for itself within several features

3. **Tests are non-negotiable for AI work**
   * Without tests: No way to validate AI suggestions
   * With tests: Instant feedback on correctness

4. **Documentation teaches AI your patterns**
   * README + ADRs â†’ AI suggests architecturally consistent code
   * No docs â†’ AI guesses (poorly)

5. **Consistency > Perfection**
   * Enforced linting (even with imperfect rules) > no enforcement
   * 30% test coverage (strategic) > 0% coverage

> [!IMPORTANT]
> **The Real Cost Comparison**
>
> **Without Fundamentals:**
>
> * Feature development: 10 days
> * Debugging: 8 days
> * Rework: 5 days
> * Incident response: 3 days
> * **Total**: 26 days + delayed launch
>
> **With Fundamentals:**
>
> * Fundamentals investment: 15 days (one-time)
> * Feature development: 5 days
> * Debugging: 0.5 days
> * Rework: 0 days
> * Incidents: 0 days
> * **Total**: 15 days + 5.5 days = 20.5 days (first feature)
> * **Second feature**: 5.5 days (breakeven after 3-4 features)

---

**Previous:** [The Four Pillars of AI-Ready Codebases](02-four-pillars.md)

**Next:** [Assessment Framework: Evaluate Your Codebase](04-assessment-framework.md)

---

<!-- markdownlint-disable MD036 -->
*ðŸ¤– Crafted with precision by âœ¨Copilot following brilliant human instruction,
then carefully refined by our team of discerning human reviewers.*
<!-- markdownlint-enable MD036 -->
